---
title: "ã€ML-Agentsã€‘UnityÃ—å¼·åŒ–å­¦ç¿’ã§ãƒ‰ãƒ­ãƒ¼ãƒ³ã‚’è‡ªå‹•æ“ç¸¦ã—ãŸã„"
emoji: "ğŸ•"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: []
published: false
---

Unity ã§ ML-Agents ã‚’ç”¨ã„ã¦ã€ãƒ‰ãƒ­ãƒ¼ãƒ³ã‚’è‡ªå‹•æ“ç¸¦ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’å¼·åŒ–å­¦ç¿’ã§ä½œæˆã—ã¾ã—ãŸã€‚

## é–‹ç™ºç’°å¢ƒ

- OS: macOS Ventura Version13.2.1
- Python 3.10.10
- ML-Agent Version release 20

## å®Œæˆãƒ¢ãƒ‡ãƒ« 

![](https://storage.googleapis.com/zenn-user-upload/51adea844e4b-20230609.gif)

ãšã£ã¨è¦‹ã¦ã‚‹ã¨é…”ã„ãã†ã€‚

## What is ML-Agents?

ML-Agentsã¨ã¯[Unity Machine Learning Agents](https://unity.com/ja/products/machine-learning-agents)ã®ã“ã¨ã§ã€Unity ã‚’ä½¿ã£ã¦æ©Ÿæ¢°å­¦ç¿’ã‚’è¡Œã†ã“ã¨ã®ã§ãã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚


ä»¥ä¸‹ã‚ˆã‚Šã€ã€ŒRelease 20ã€ã®ã€Œdownloadã€ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Unityã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

https://github.com/Unity-Technologies/ml-agents

## What is å¼·åŒ–å­¦ç¿’?


## ML-Agentsã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’

### å¼·åŒ–å­¦ç¿’ã®å„è¦ç´ 

| é …ç›® | å†…å®¹ |
| --- | --- |
| è¡Œå‹• | ãƒ‰ãƒ­ãƒ¼ãƒ³ã®å·¦å³ç§»å‹•/å‰å¾Œç§»å‹•/ä¸Šæ˜‡/ä¸‹é™/å›è»¢ |
| å ±é…¬ | åŠ ç®—ï¼šãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®é€šéã€æ¸›ç®—ï¼šå£ã¸ã®è¡çª |
| è¦³å¯Ÿ | ãƒ‰ãƒ­ãƒ¼ãƒ³ã®é€Ÿåº¦/å›è»¢ |

### Playerã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```cs
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Actuators;
using Unity.ãƒãƒãƒ¼ã‚¸ãƒ£MLAgents.Sensors;

// This is a public class called Player that inherits from another class called Agent
public class Player : Agent
{
    // These are public fields used to control the player's movement and tilt
    public float moveSpeed = 20f;
    public float rotSpeed = 100f;
    public float verticalForce = 17f;
    public float forwardTiltAmount = 0;
    public float sidewaysTiltAmount = 0;
    public float tiltVel = 2f;

    // These are private fields used for physics calculations and keeping track of checkpoints hit
    private Rigidbody playerRb;
    private float tiltAng = 45f;
    private int cptCount;

    // This method is called once when the agent is initialized. It finds the Rigidbody component on the game object.
    public override void Initialize()
    {
        playerRb = GetComponent<Rigidbody>();
    }

    // This method is called each time a new episode begins. It resets the player's position and velocity, as well as the checkpoint count.
    public override void OnEpisodeBegin()
    {
        cptCount = 0;
        transform.position = new Vector3(0, 10.0f, 0);
        transform.rotation = Quaternion.identity;
        playerRb.velocity = Vector3.zero;
        playerRb.angularVelocity = Vector3.zero;

        // Applies force to the player in the direction of (0, 200, 200) in global space.
        playerRb.AddForce(transform.TransformDirection(new Vector3(0, 200.0f, 200.0f)));
    }

    // This method is called whenever the player collides with a trigger collider.
    private void OnTriggerEnter(Collider other)
    {
        // If the collider has the tag "Checkpoint", it awards a reward and increases the checkpoint count. If the count reaches 4, it ends the episode and awards more points.
        if (other.CompareTag("CheckPoint"))
        {
            AddReward(1.0f);
            cptCount++;
            if (cptCount >= 4)
            {
                AddReward(10.0f);
                EndEpisode();
            }
        }
        // If the collider has the tag "Wall", it penalizes the player and ends the episode.
        else if (other.CompareTag("Wall"))
        {
            AddReward(-5.0f);
            EndEpisode();
        }
    }

    // This method is called each time the agent senses its environment, such as when the observation data is collected for the neural network.
    public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(playerRb.velocity);
        sensor.AddObservation(transform.rotation.eulerAngles);
    }

    // This method is called each time the agent receives an action from the neural network.
    public override void OnActionReceived(ActionBuffers actions)
    {
        // These are the continuous actions received in the buffer
        float horInput = actions.ContinuousActions[0];
        float verInput = actions.ContinuousActions[1];
        float upInput = actions.ContinuousActions[2];
        float downInput = actions.ContinuousActions[3];
        float rotInput = actions.ContinuousActions[4];

        // Calculates and applies force to move the player based on the input values.
        Vector3 moveDirection = new Vector3(horInput, 0, verInput) * moveSpeed;
        playerRb.AddForce(transform.TransformDirection(moveDirection));

        // Applies upward or downward force to the player based on the input values.
        if (upInput > 0)
        {
            playerRb.AddForce(Vector3.up * verticalForce * upInput);
        }

        if (downInput > 0)
        {
            playerRb.AddForce(Vector3.down * verticalForce * downInput);
        }

        // Applies rotational force to the player based on the input values.
        transform.Rotate(0, rotInput * rotSpeed * Time.fixedDeltaTime, 0);

        // Applies forward and sideways tilt to the player based on the input values.
        sidewaysTiltAmount = Mathf.Lerp(sidewaysTiltAmount, -horInput * tiltAng, tiltVel * Time.fixedDeltaTime);
        forwardTiltAmount = Mathf.Lerp(forwardTiltAmount, verInput * tiltAng, tiltVel * Time.fixedDeltaTime);

        Quaternion targetRot = Quaternion.Euler(forwardTiltAmount, 0, sidewaysTiltAmount);
        transform.localRotation = targetRot;
    }

    // This method is called during training when using a player-controlled agent. It maps user input to the action buffer.
    public override void Heuristic(in ActionBuffers actionsOut)
    {
        float horInput = Input.GetAxis("Horizontal");
        float verInput = Input.GetAxis("Vertical");
        float upInput = Input.GetKey(KeyCode.Space) ? 1 : 0;
        float downInput = Input.GetKey(KeyCode.LeftControl) ? 1 : 0;
        float rotInput = Input.GetAxis("Mouse X");

        ActionSegment<float> continuousAct = actionsOut.ContinuousActions;
        continuousAct[0] = horInput;
        continuousAct[1] = verInput;
        continuousAct[2] = upInput;
        continuousAct[3] = downInput;
        continuousAct[4] = rotInput;
    }
}
```
### è¡çªæ¤œå‡º

1. å£ã®ã‚¿ã‚°ã‚’`Wall`ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¿ã‚°ã‚’`CheckPoint`ã«è¨­å®šã—ã¦ã€ã‚³ãƒ¼ã‚¹ã‚’ä½œã‚Šã¾ã™ã€‚ä¸Šã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€4ã¤ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚
1. PlayerãŒã‚²ãƒ¼ãƒ å†…ã®ãƒˆãƒªã‚¬ãƒ¼ã‚³ãƒ©ã‚¤ãƒ€ãƒ¼ã¨è¡çªã™ã‚‹ãŸã³ã«ã€`OnTriggerEnter`ãƒ¡ã‚½ãƒƒãƒ‰ãŒå‘¼ã³å‡ºã•ã‚Œã€ã‚¿ã‚°ãŒ`CheckPoint`ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¡çªã—ãŸå ´åˆã€å ±é…¬ã‚’è¿½åŠ ã—ã€`cptCount`ã‚’å¢—ã‚„ã—ã¾ã™ã€‚
1. ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’é€šéã—ãŸå ´åˆã€ã‚´ãƒ¼ãƒ«å ±é…¬ã‚’è¿½åŠ ã—ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™ã€‚
ä¸€æ–¹ã€ã‚¿ã‚°ãŒ`Wall`ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¡çªã—ãŸå ´åˆã€ãƒšãƒŠãƒ«ãƒ†ã‚£å ±é…¬ã‚’è¿½åŠ ã—ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™ã€‚

## å­¦ç¿’ã‚’é–‹å§‹ã™ã‚‹
### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è¨­å®š
ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸã€Œml-agents-release_20ã€ãƒ•ã‚©ãƒ«ãƒ€ã®ã€Œconfigã€ãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã«ã€ã€Œdrone_training.yamlã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã‚’è¨˜è¿°ã—ã¾ã™ã€‚

```yaml
behaviors:
  DroneAgent:
    trainer_type: ppo # Use PPO (Proximal Policy Optimization) for training algorithm
    hyperparameters:
      batch_size: 128 
      buffer_size: 2048 # Experience Replay Buffer Size
      learning_rate: 3.0e-5 
      beta: 0.01 # Target value of KL divergence
      epsilon: 0.2 # Clipping parameters for PPO
      lambd: 0.95 # GAE (Generalized Advantage Estimation) lambda parameter
      num_epoch: 3 # epoch number
      learning_rate_schedule: linear # Scheduling Methods for Learning Rates
    network_settings:
      normalize: false # No input normalization
      hidden_units: 1024 # Number of hidden layer units
      num_layers: 3 # Number of hidden layers
    reward_signals:
      extrinsic:
        gamma: 0.99 # Discount rate of compensation
        strength: 1.0 # Reward weighting
    keep_checkpoints: 5 # Number of checkpoints to save
    max_steps: 5.0e5 # Maximum number of training steps
    time_horizon: 128 # Maximum number of time steps during training
    summary_freq: 10000 # Frequency of summary output
    threaded: true # Enable parallel training
```
### é–‹ç™ºç’°å¢ƒã®æ§‹ç¯‰
ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚ˆã‚Šã€pythonã®é–‹ç™ºç’°å¢ƒã‚’é–‹å§‹ã—ã¾ã™ã€‚
```
python -m venv env
```
ä½œæˆã—ãŸç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«ã—ã¾ã™ã€‚
```
source env/bin/activate
```
ml-agentsã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è¡Œã„ã¾ã™ã€‚
```
pip install mlagents
```
`ml-agents-release_20`ãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•ã—ã¦ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
```
mlagents-learn config/drone_training.yaml --run-id=drone_1
```

Unityä¸Šã§ã€â–¶ï¸ã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚

#### å‚è€ƒã«ã•ã›ã¦é ‚ã„ãŸè¨˜äº‹
https://axross-recipe.com/users/814
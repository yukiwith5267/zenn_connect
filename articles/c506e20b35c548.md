---
title: "ã€ML-Agentsã€‘UnityÃ—å¼·åŒ–å­¦ç¿’ã§ãƒ‰ãƒ­ãƒ¼ãƒ³ã‚’è‡ªå‹•æ“ç¸¦ã—ãŸã„"
emoji: "ğŸ‡"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: []
published: false
---
## å®Œæˆãƒ¢ãƒ‡ãƒ«

![](https://storage.googleapis.com/zenn-user-upload/51adea844e4b-20230609.gif)
ãšã£ã¨è¦‹ã¦ã‚‹ã¨é…”ã„ãã†

## é–‹ç™ºç’°å¢ƒ

- OS: macOS Ventura Version13.2.1
- ML-Agent Version release 20


## What is ML-Agents?

ML-Agentsã¨ã¯[Unity Machine Learning Agents](https://unity.com/ja/products/machine-learning-agents)ã®ã“ã¨ã§ã€Unity ã‚’ä½¿ã£ã¦æ©Ÿæ¢°å­¦ç¿’ã‚’è¡Œã†ã“ã¨ã®ã§ãã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚


ä»¥ä¸‹ã‚ˆã‚Šã€ã€ŒRelease 20ã€ã®ã€Œdownloadã€ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Unityã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

https://github.com/Unity-Technologies/ml-agents

## ML-Agentsã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’

### 1. å¼·åŒ–å­¦ç¿’ã®å„è¦ç´ 

| é …ç›® | å†…å®¹ |
| --- | --- |
| è¡Œå‹• | ãƒ‰ãƒ­ãƒ¼ãƒ³ã®å·¦å³ç§»å‹•/å‰å¾Œç§»å‹• / ä¸Šæ˜‡ / ä¸‹é™ / å›è»¢ |
| å ±é…¬ | åŠ ç®—ï¼šãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®é€šéã€æ¸›ç®—ï¼šå£ã¸ã®è¡çª |
| è¦³å¯Ÿ | ãƒ‰ãƒ­ãƒ¼ãƒ³ã®Rigidbodyã®é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ« / ç¾åœ¨ã®å›è»¢è§’åº¦ |

### 2. å­¦ç¿’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
ã€Œå­¦ç¿’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã€ã¯ã€å­¦ç¿’ã«åˆ©ç”¨ã™ã‚‹ã€Œãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã‚’è¨­å®šã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚ã€Œãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã¨ã¯ã€å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æŒã¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸­ã§äººãŒèª¿ç¯€ã—ãªã„ã¨ã„ã‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã“ã¨ã§ã™ã€‚
ä»Šå›ã¯ã€PPOã§å­¦ç¿’ã—ã¾ã—ãŸã€‚
ã€Œml-agents-release_20ã€ãƒ•ã‚©ãƒ«ãƒ€ã®ã€Œconfigã€ãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã«ã€ã€Œdrone_training.yamlã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã‚’è¨˜è¿°ã—ã¾ã™ã€‚

```yaml:config/drone_training.yaml
behaviors:
  DroneAgent:
    trainer_type: ppo # Use PPO (Proximal Policy Optimization) for training algorithm
    hyperparameters:
      batch_size: 128 
      buffer_size: 2048 # Experience Replay Buffer Size
      learning_rate: 3.0e-5 
      beta: 0.01 # Target value of KL divergence
      epsilon: 0.2 # Clipping parameters for PPO
      lambd: 0.95 # GAE (Generalized Advantage Estimation) lambda parameter
      num_epoch: 3 # epoch number
      learning_rate_schedule: linear # Scheduling Methods for Learning Rates
    network_settings:
      normalize: false # No input normalization
      hidden_units: 1024 # Number of hidden layer units
      num_layers: 3 # Number of hidden layers
    reward_signals:
      extrinsic:
        gamma: 0.99 # Discount rate of compensation
        strength: 1.0 # Reward weighting
    keep_checkpoints: 5 # Number of checkpoints to save
    max_steps: 5.0e5 # Maximum number of training steps
    time_horizon: 128 # Maximum number of time steps during training
    summary_freq: 10000 # Frequency of summary output
    threaded: true # Enable parallel training
```

### 2. Playerã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```cs:Player.cs
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Actuators;
using Unity.MLAgents.Sensors;

// This is a public class called Player that inherits from another class called Agent
public class Player : Agent
{
    // These are public fields used to control the player's movement and tilt
    public float moveSpeed = 20f;
    public float rotSpeed = 100f;
    public float verticalForce = 20f;
    public float forwardTiltAmount = 0;
    public float sidewaysTiltAmount = 0;
    public float tiltVel = 2f;

    // These are private fields used for physics calculations and keeping track of checkpoints hit
    private Rigidbody playerRb;
    private float tiltAng = 45f;
    private int cptCount;

    // This method is called once when the agent is initialized. It finds the Rigidbody component on the game object.
    public override void Initialize()
    {
        playerRb = GetComponent<Rigidbody>();
    }

    // This method is called each time a new episode begins. It resets the player's position and velocity, as well as the checkpoint count.
    public override void OnEpisodeBegin()
    {
        cptCount = 0;
        transform.position = new Vector3(0, 10.0f, 0);
        transform.rotation = Quaternion.identity;
        playerRb.velocity = Vector3.zero;
        playerRb.angularVelocity = Vector3.zero;

        // Applies force to the player in the direction of (0, 200, 200) in global space.
        playerRb.AddForce(transform.TransformDirection(new Vector3(0, 200.0f, 200.0f)));
    }

    // This method is called whenever the player collides with a trigger collider.
    private void OnTriggerEnter(Collider other)
    {
        // If the collider has the tag "Checkpoint", it awards a reward and increases the checkpoint count. If the count reaches 4, it ends the episode and awards more points.
        if (other.CompareTag("CheckPoint"))
        {
            AddReward(1.0f);
            cptCount++;
            if (cptCount >= 4)
            {
                AddReward(10.0f);
                EndEpisode();
            }
        }
        // If the collider has the tag "Wall", it penalizes the player and ends the episode.
        else if (other.CompareTag("Wall"))
        {
            AddReward(-5.0f);
            EndEpisode();
        }
    }

    // This method is called each time the agent senses its environment, such as when the observation data is collected for the neural network.
    public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(playerRb.velocity);
        sensor.AddObservation(transform.rotation.eulerAngles);
    }

    // This method is called each time the agent receives an action from the neural network.
    public override void OnActionReceived(ActionBuffers actions)
    {
        // These are the continuous actions received in the buffer
        float horInput = actions.ContinuousActions[0];
        float verInput = actions.ContinuousActions[1];
        float upInput = actions.ContinuousActions[2];
        float downInput = actions.ContinuousActions[3];
        float rotInput = actions.ContinuousActions[4];

        // Calculates and applies force to move the player based on the input values.
        Vector3 moveDirection = new Vector3(horInput, 0, verInput) * moveSpeed;
        playerRb.AddForce(transform.TransformDirection(moveDirection));

        // Applies upward or downward force to the player based on the input values.
        if (upInput > 0)
        {
            playerRb.AddForce(Vector3.up * verticalForce * upInput);
        }

        if (downInput > 0)
        {
            playerRb.AddForce(Vector3.down * verticalForce * downInput);
        }

        // Applies rotational force to the player based on the input values.
        transform.Rotate(0, rotInput * rotSpeed * Time.fixedDeltaTime, 0);

        // Applies forward and sideways tilt to the player based on the input values.
        sidewaysTiltAmount = Mathf.Lerp(sidewaysTiltAmount, -horInput * tiltAng, tiltVel * Time.fixedDeltaTime);
        forwardTiltAmount = Mathf.Lerp(forwardTiltAmount, verInput * tiltAng, tiltVel * Time.fixedDeltaTime);

        Quaternion targetRot = Quaternion.Euler(forwardTiltAmount, 0, sidewaysTiltAmount);
        transform.localRotation = targetRot;
    }

    // This method is called during training when using a player-controlled agent. It maps user input to the action buffer.
    public override void Heuristic(in ActionBuffers actionsOut)
    {
        float horInput = Input.GetAxis("Horizontal");
        float verInput = Input.GetAxis("Vertical");
        float upInput = Input.GetKey(KeyCode.Space) ? 1 : 0;
        float downInput = Input.GetKey(KeyCode.LeftControl) ? 1 : 0;
        float rotInput = Input.GetAxis("Mouse X");

        ActionSegment<float> continuousAct = actionsOut.ContinuousActions;
        continuousAct[0] = horInput;
        continuousAct[1] = verInput;
        continuousAct[2] = upInput;
        continuousAct[3] = downInput;
        continuousAct[4] = rotInput;
    }
}
```
1. å£ã®ã‚¿ã‚°ã‚’`Wall`ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¿ã‚°ã‚’`CheckPoint`ã«è¨­å®šã—ã¦ã€ã‚³ãƒ¼ã‚¹ã‚’ä½œã‚Šã¾ã™ã€‚
1. PlayerãŒã‚²ãƒ¼ãƒ å†…ã®ãƒˆãƒªã‚¬ãƒ¼ã‚³ãƒ©ã‚¤ãƒ€ãƒ¼ã¨è¡çªã™ã‚‹ãŸã³ã«ã€`OnTriggerEnter`ãƒ¡ã‚½ãƒƒãƒ‰ãŒå‘¼ã³å‡ºã•ã‚Œã€ã‚¿ã‚°ãŒ`CheckPoint`ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¡çªã—ãŸå ´åˆã€å ±é…¬ã‚’è¿½åŠ ã—ã€`cptCount`ã‚’å¢—ã‚„ã—ã¾ã™ã€‚
1. ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’é€šéã—ãŸå ´åˆã€ã‚´ãƒ¼ãƒ«å ±é…¬ã‚’è¿½åŠ ã—ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™ã€‚
ä¸€æ–¹ã€ã‚¿ã‚°ãŒ`Wall`ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¡çªã—ãŸå ´åˆã€ãƒšãƒŠãƒ«ãƒ†ã‚£å ±é…¬ã‚’è¿½åŠ ã—ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™ã€‚

## å­¦ç¿’ã‚’é–‹å§‹ã™ã‚‹

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚ˆã‚Šã€pythonã®é–‹ç™ºç’°å¢ƒã‚’é–‹å§‹ã—ã¾ã™ã€‚
```
python -m venv env
```
ä½œæˆã—ãŸç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«ã—ã¾ã™ã€‚
```
source env/bin/activate
```
ml-agentsã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è¡Œã„ã¾ã™ã€‚
```
pip install mlagents
```
`ml-agents-release_20`ãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•ã—ã¦ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
```
mlagents-learn config/drone_training.yaml --run-id=drone_1
```

## å­¦ç¿’çµæœ


Tensorboardã§å ±é…¬ã®æ¨ç§»ã‚’ç¢ºèªã€‚20ä¸‡å›ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã€æœ€å¤§å ±é…¬ã®14ã«åæŸã—ãŸã€‚
![](https://storage.googleapis.com/zenn-user-upload/fe8020c6dcf9-20230612.png)

## æ„Ÿæƒ³
å­¦ç¿’ç‡ã‚’é«˜ãã™ã‚‹ã¨ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°é‡ãŒå¤§ãããªã‚‹ãŸã‚ã€åæŸã¾ã§ã®æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ã—ã‹ã—ã€å­¦ç¿’ç‡ãŒéå¸¸ã«é«˜ã„å ´åˆã€æ›´æ–°é‡ãŒå¤§ãã™ãã¦ç›®çš„ã®æœ€é©è§£ã‚’é€šã‚Šè¶Šã—ã¦ã€å±€æ‰€çš„ãªæœ€é©è§£ã«è½ã¡ç€ã„ã¦ã—ã¾ã†ã€‚
ä¸€æ–¹ã€å­¦ç¿’ç‡ã‚’ä½ãã™ã‚‹ã¨ã€æ›´æ–°é‡ãŒå°ã•ããªã‚‹ãŸã‚ã€åæŸã¾ã§ã®æ™‚é–“ã¯é•·ããªã‚‹ãŒã€å±€æ‰€çš„ãªæœ€é©è§£ã‹ã‚‰è„±å‡ºã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
å­¦ç¿’ç‡ã‚’é©åˆ‡ãªå€¤ã«èª¿æ•´ã™ã‚‹ã®ãŒé›£ã—ã‹ã£ãŸã€‚

## å‚è€ƒæ–‡çŒ®
- [Unityã§ã¯ã˜ã‚ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ»å¼·åŒ–å­¦ç¿’ Unity ML-Agents å®Ÿè·µã‚²ãƒ¼ãƒ ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° v2.2å¯¾å¿œç‰ˆ](https://www.amazon.co.jp/Unityã§ã¯ã˜ã‚ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ»å¼·åŒ–å­¦ç¿’-Unity-ML-Agents-å®Ÿè·µã‚²ãƒ¼ãƒ ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°-v2-2å¯¾å¿œç‰ˆ/dp/4862465447/ref=sr_1_3?crid=1JA6QMM1EOMPD&keywords=unity+ml-agents&qid=1686540743&sprefix=unity+ml%2Caps%2C290&sr=8-3)
- [https://axross-recipe.com/recipes/1055](https://axross-recipe.com/recipes/1055)

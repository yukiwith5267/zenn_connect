---
title: "【Requests+BeautifulSoup, Selenium】メモ置き場"
emoji: "🐙"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

### 動機

大学の課題を忘れるということが多々あるので、自動で課題の締め切りを通知してくれるプログラムを作りたいと思った。GPTに聞いたら、Webスクレイピングを使えばいいということになったので、Webスクレイピングを勉強することにした。
以下メモ置き場として使う。

### ライブラリ


|      |  ①データの収集  |　②データの抽出  |　③データの保存  |
| ---- | ---- | ---- | ---- |
|  Requests  |  ○  |　　  |     |
|  BeautifulSoup |    |  ○  |   |
|  Selenium  |  ○  |  ○  |    |
|  Pandas  |    |    |  ○  |

一般的なwebサイトでは、`Requests`+`BeautifulSoup`で十分だが、JavaScriptで動的にページが生成されるサイトでは、`Selenium`を使う必要がある。

### Requestsの使い方(データの収集)

```python
# ライブラリのインポート
import requests

# URLの指定
url = "https://www.python.org"

# requests.get()でURLのHTMLを取得
r = requests.get(url)

# HTMLを表示
print(r.text)

# ステータスコードを表示
print(r.status_code)

#200->sucsess
#300->redirect
#400->client error
#500->server error

```

- `requests.get(url)`で、指定したURLにアクセスできる
- アクセス結果の確認
   - `r.text` : str型で取得
   - `r.content` : bytes型で取得
   - `r.raise_for_status()` : ステータスコードに応じてエラーを発生させる

### BeautifulSoupの使い方(データの解析)

```python
import requests

# BeautifulSoupのインポート
from bs4 import BeautifulSoup

url = "https://www.python.org"
r = requests.get(url)

# BeautifulSoupでHTMLを解析
soup = BeautifulSoup(r.content, "lxml")
# h1タグの要素を表示
print(soup.find("h1"))
```
- HTMLの解析方法
```python
soup = BeautifulSoup(r.context, "lxml")#解析した結果をsoupに格納
```
- HTMLタグのidやclassを指定して要素を取得する

```python
soup.find('タグ名')#指定したタグの最初の要素を取得
soup.find_all('タグ名')#指定したタグの全ての要素を取得
soup.find(id='id名')#指定したidの要素を取得
soup.find(class_='class名')#指定したclassの要素を取得
soup.find(text=re.compile('正規表現'))#正規表現で指定した要素を取得
```

- タグから情報を抽出する
  
```python
find().text#指定した要素からテキストを取得
find().get('href')#指定した要素からhref属性の値を取得
```

### スクレイピングしたデータをCSV・Excelに出力する

```python
import requests
import pandas as pd
from bs4 import BeautifulSoup

url = "https://python.org/"
r = requests.get(url)

soup = BeautifulSoup(r.content, "lxml")
post = soup.find('div', class_='shrubbery')

d_list = []
for li in (post.find_all('li')):
    d = {
        'title': li.find('a').text,
        'time': li.find('time').text,
        'url': li.find('a').get('href')
    }
    d_list.append(d)  #d_listに辞書を追加する

df = pd.DataFrame(d_list)
df.to_csv('python.csv', index=None, encoding='utf-8-sig')
df.to_excel('python.xlsx', index=None)
```

---
title: "ã€Requests+BeautifulSoup, Seleniumã€‘ãƒ¡ãƒ¢ç½®ãå ´"
emoji: "ğŸ™"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: []
published: false
---

### å‹•æ©Ÿ

å¤§å­¦ã®èª²é¡Œã‚’å¿˜ã‚Œã‚‹ã¨ã„ã†ã“ã¨ãŒå¤šã€…ã‚ã‚‹ã®ã§ã€è‡ªå‹•ã§èª²é¡Œã®ç· ã‚åˆ‡ã‚Šã‚’é€šçŸ¥ã—ã¦ãã‚Œã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œã‚ŠãŸã„ã¨æ€ã£ãŸã€‚GPTã«èã„ãŸã‚‰ã€Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ä½¿ãˆã°ã„ã„ã¨ã„ã†ã“ã¨ã«ãªã£ãŸã®ã§ã€Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å‹‰å¼·ã™ã‚‹ã“ã¨ã«ã—ãŸã€‚
ä»¥ä¸‹ãƒ¡ãƒ¢ç½®ãå ´ã¨ã—ã¦ä½¿ã†ã€‚

### ãƒ©ã‚¤ãƒ–ãƒ©ãƒª


|      |  â‘ ãƒ‡ãƒ¼ã‚¿ã®åé›†  |ã€€â‘¡ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º  |ã€€â‘¢ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜  |
| ---- | ---- | ---- | ---- |
|  Requests  |  â—‹  |ã€€ã€€  |     |
|  BeautifulSoup |    |  â—‹  |   |
|  Selenium  |  â—‹  |  â—‹  |    |
|  Pandas  |    |    |  â—‹  |

ä¸€èˆ¬çš„ãªwebã‚µã‚¤ãƒˆã§ã¯ã€`Requests`+`BeautifulSoup`ã§ååˆ†ã ãŒã€JavaScriptã§å‹•çš„ã«ãƒšãƒ¼ã‚¸ãŒç”Ÿæˆã•ã‚Œã‚‹ã‚µã‚¤ãƒˆã§ã¯ã€`Selenium`ã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã€‚

### Requestsã®ä½¿ã„æ–¹(ãƒ‡ãƒ¼ã‚¿ã®åé›†)

```python
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import requests

# URLã®æŒ‡å®š
url = "https://www.python.org"

# requests.get()ã§URLã®HTMLã‚’å–å¾—
r = requests.get(url)

# HTMLã‚’è¡¨ç¤º
print(r.text)

# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
print(r.status_code)

#200->sucsess
#300->redirect
#400->client error
#500->server error

```

- `requests.get(url)`ã§ã€æŒ‡å®šã—ãŸURLã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹
- ã‚¢ã‚¯ã‚»ã‚¹çµæœã®ç¢ºèª
   - `r.text` : strå‹ã§å–å¾—
   - `r.content` : byteså‹ã§å–å¾—
   - `r.raise_for_status()` : ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã«å¿œã˜ã¦ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹

### BeautifulSoupã®ä½¿ã„æ–¹(ãƒ‡ãƒ¼ã‚¿ã®è§£æ)

```python
import requests

# BeautifulSoupã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from bs4 import BeautifulSoup

url = "https://www.python.org"
r = requests.get(url)

# BeautifulSoupã§HTMLã‚’è§£æ
soup = BeautifulSoup(r.content, "lxml")
# h1ã‚¿ã‚°ã®è¦ç´ ã‚’è¡¨ç¤º
print(soup.find("h1"))
```
- HTMLã®è§£ææ–¹æ³•
```python
soup = BeautifulSoup(r.context, "lxml")#è§£æã—ãŸçµæœã‚’soupã«æ ¼ç´
```
- HTMLã‚¿ã‚°ã®idã‚„classã‚’æŒ‡å®šã—ã¦è¦ç´ ã‚’å–å¾—ã™ã‚‹

```python
soup.find('ã‚¿ã‚°å')#æŒ‡å®šã—ãŸã‚¿ã‚°ã®æœ€åˆã®è¦ç´ ã‚’å–å¾—
soup.find_all('ã‚¿ã‚°å')#æŒ‡å®šã—ãŸã‚¿ã‚°ã®å…¨ã¦ã®è¦ç´ ã‚’å–å¾—
soup.find(id='idå')#æŒ‡å®šã—ãŸidã®è¦ç´ ã‚’å–å¾—
soup.find(class_='classå')#æŒ‡å®šã—ãŸclassã®è¦ç´ ã‚’å–å¾—
soup.find(text=re.compile('æ­£è¦è¡¨ç¾'))#æ­£è¦è¡¨ç¾ã§æŒ‡å®šã—ãŸè¦ç´ ã‚’å–å¾—
```

- ã‚¿ã‚°ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹
  
```python
find().text#æŒ‡å®šã—ãŸè¦ç´ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
find().get('href')#æŒ‡å®šã—ãŸè¦ç´ ã‹ã‚‰hrefå±æ€§ã®å€¤ã‚’å–å¾—
```

### ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ»Excelã«å‡ºåŠ›ã™ã‚‹

```python
import requests
import pandas as pd
from bs4 import BeautifulSoup

url = "https://python.org/"
r = requests.get(url)

soup = BeautifulSoup(r.content, "lxml")
post = soup.find('div', class_='shrubbery')

d_list = []
for li in (post.find_all('li')):
    d = {
        'title': li.find('a').text,
        'time': li.find('time').text,
        'url': li.find('a').get('href')
    }
    d_list.append(d)  #d_listã«è¾æ›¸ã‚’è¿½åŠ ã™ã‚‹

df = pd.DataFrame(d_list)
df.to_csv('python.csv', index=None, encoding='utf-8-sig')
df.to_excel('python.xlsx', index=None)
```
